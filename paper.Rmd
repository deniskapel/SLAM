---
title: "Second Language Acquisition: Exploring Common Mistakes"
author: "Denis Kapelyushnik^[HSE University, dmkapelyushnik@edu.hse.ru, https://github.com/deniskapel/SLAM]"
abstract: "In 2018, a challenge on Second Language Acquisition Modeling was organised by Duolingo AI in conjunction with the 13th BEA Workshop and NAACL-HLT 2018 conference. One of the key findings of the challenge was the fact that a choice of a learning algorithm (for the task) appears to be more important than clever feature engineering. This research paper for the Linguistic Data: Quantitative Analysis and Visualisation course is aimed to explore if any connection between certain available features and mistakes made while acquiring a foreign language exists."
bibliography: references.bib
output:
  pdf_document: default
  bibliography: default
---

```{r, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, results='hide')
```

```{r}
library(tidyverse)
library(patchwork)
library(tidytext)
library(lme4)
library(lmerTest)
```

## 1. Metadata

The dataset used for this paper comes from @slam18. 7M words produced by more than 6k learners of English, Spanish, and French using Duolingo, an online language-learning app, were collected for the Second Language Acquisition Modeling (SLAM) task. The more detailed task description and results achieved by contestants are available on the official task page^[http://sharedtask.duolingo.com/2018].

The original data is organized into language pairs: `es_en` — Spanish learners (who already speak English), `fr_en` — French learners (who already speak English), `en_es` English learners  (who already speak SPanish). This project is focused on French learners only.

Only `train` splits prepared by @DVN/8SWHNO_2018 were used in this project. A dataset per language pair was split into two files^[ To reproduce this paper, follow the instructions specified in the data folder of the project github repository: https://github.com/deniskapel/SLAM/tree/main/data]: `fr_en_metadata.csv` and `fr_en_sessions.csv`. 

Both files contain data separated by tabs (no headers):

``` {r, results='markup'}
cols_md = c('user_id', 'country', 'days', 'client',
            'session', 'format', 'time', 'session_id',
            'n_tokens', 'n_errors', 'prompt')
col_types_md = cols('f','f','d','f','f','f','i','f','i', 'i', 'f')

fr_en_md <- read_tsv("data/fr_en_metadata.csv", 
                     col_names = cols_md,
                     col_types = col_types_md)

data_description_md <- c(
  'generated during data anonimisation',
  'a 2-character country code',
  'day of usage (a double)',
  'android, ios, or web',
  'lesson, practice, or test',
  'reverse_translate, reverse_tap, or listen',
  'duration of the answer in seconds',
  'use it to join metadata and sessions',
  'a number of tokens used in the task',
  'a number of mistakes a user made',
  'prompt (no prompt in listening)'
)

knitr::kable(data.frame(cols_md, data_description_md),
             col.names = c("Column name", 'Description'),
             align = c('l','l'),
             caption = "Content of the *_metada.csv files")
```

``` {r, echo=FALSE, results='markup'}
cols_ses = c('session_id', 
             'task_token_id', 'token', 'POS', 
             'morph', 'ud_edge_label', 
             'ud_edge_head', 'label')

col_types_ses = cols('f','i','f','f','c','f','f','i')

fr_en_sessions <- read_tsv("data/fr_en_sessions.csv", 
                     col_names = cols_ses,
                     col_types = col_types_ses)

data_description_ses <- c(
  'unique ID for a sesssion',
  'location of a token in a task','word','part of speech in UD format',
  'morphological features in UD format','dependency edge label in UD format',
  'dependency edge head in UD format',
  'to be predicted (0 or 1): 0 - correct, 1 - wrong'
)

knitr::kable(data.frame(cols_ses, data_description_ses),
             col.names = c("Column name", 'Description'),
             align = c('l','l'),
             caption = "Content of the *_sessions.csv files")
```


## 2. Describing the data

### 2.1. Countries and users

Overall, there are more than 100 locations where people use the app and the number of users in these countries can differ significantly. 

```{r, fig.height=3, fig.width=10}
fr_top10 <- fr_en_md %>% 
  select(country, user_id) %>% 
  group_by(country) %>% 
  summarise(n_users = n()) %>% 
  slice_max(n_users, n=10)

fr_users <- fr_top10 %>% 
  ggplot(aes(x=country, y=n_users / 100)) +
  geom_col() +
  labs(title = "Top 10 most represented countries",
       x = "",
       y = "",
       caption="Y axis = 1 unit = 100 students")+
  theme_minimal()

fr_errors_by_country <- fr_en_md %>% 
  filter(country %in% fr_top10$country) %>% 
  select(n_errors, country, user_id) %>% 
  group_by(country, user_id) %>% 
  summarise(avg_errors = mean(n_errors) * 100, .groups="drop") %>% 
  ggplot(aes(x=country, y=avg_errors))+
  geom_boxplot()+
  labs(title="Average number of errors per location", 
       y = "",
       x = "",
       caption="first 30 days of using the app")+
  theme_minimal()

fr_errors_by_country + fr_users
```

The left side of the graph above demonstrates similarity in a number of mistakes per user (with their mean slightly below 50) in 10 most represented countries for a 30-day period. Based on this, it was decided to limit the data for the project to three countries from Top 4 most represented countries to eliminate any additional factors (e.g. L1) that might have influence on second language acquisition (SLA). Users from `Canada`, `Great Britain` and `Australia` are assumed to be native speakers of English. Additionally, it will be interesting to check if Canada' bilingual status has any influence on SLA. `USA` is removed from the project, mainly to save on computational resources - there are almost as many users from this country as from other three altogether. 

```{r}
# filter the dataset to top 4 most represented countries
fr_en_md <- fr_en_md %>% 
  filter(country %in% c('CA', 'GB', 'AU'))

fr_en_sessions <- fr_en_sessions %>% 
  filter(session_id %in% fr_en_md$session_id)

fr_en_md %>% 
  select(session_id) %>% 
  distinct() %>% 
  count() == (fr_en_sessions %>% 
                select(session_id) %>% 
                distinct() %>% 
                count())
```

Allegedly, all the users are beginners who are taking first steps in acquiring L2. Mostly, they start using the app actively but their engagement decreases over time.

```{r, results='markup', fig.height=3, fig.width=10}
fr_en_md %>% 
  select(country, client, user_id, days) %>% 
  mutate(days = round(days)) %>% 
  group_by(country, user_id, days) %>% 
  summarise(n_sessions = n(), .groups="drop") %>% 
  group_by(country, days) %>% 
  summarise(avg_sessions = round(mean(n_sessions), 2),  .groups="drop") %>% 
  ggplot(aes(x=days, y=avg_sessions)) +
  geom_line()+
  geom_point()+
  geom_smooth(se = FALSE, method=lm)+
  facet_wrap(~country)+
  labs(x="Days", y="mean N sessions",
       title="Average students' engagement over 30 days of practice")+
  theme_minimal()
```

As for users' social status, `client` (users' devices) is the only feature that might be used to describe it (quite indirectly, though). In general, all the users come from high-income countries, and there is no obvious reason to start learning French there (except for `Canada`).

```{r, results='markup', fig.height=3}
fr_en_md %>% 
  select(country, client) %>%
  group_by(country, client) %>% 
  summarise(n_users = n(),  .groups="drop") %>% 
  group_by(country) %>% 
  summarise(prct = round(n_users/sum(n_users),1),
            client = client,  .groups="drop") %>% 
  ggplot(aes(x = country, y = prct, fill=client)) + 
  geom_col(position = "dodge")+
  scale_y_continuous(labels = scales::percent)+
  scale_fill_grey(start=0.8, end=0.2)+
  labs(y="", x="", fill="User's device") +
  theme_minimal()
```

### 2.2. Types of sessions

There are session types in the dataset: `lesson`, `practice` and `test`. The `lesson` sessions are where new words or concepts are introduced, although lessons also include a lot of previously-learned material (e.g., each exercise tries to introduce only one new word or tense, so all other tokens should have been seen by the student before). The `practice` sessions should contain only previously-seen words and concepts. The `test` sessions allow a student "skip" a particular skill unit of the curriculum (i.e., the student may have never seen this content before in the Duolingo app, but may well have had prior knowledge before starting the course).

```{r, results='markup', fig.width=10, fig.height=5}
session_types <- fr_en_md %>% 
  ggplot() +
  geom_bar(aes(x="", fill=session)) +
  coord_polar(theta = "y")+
  scale_fill_grey(start=0.8, end=0) +
  theme_void() +
  labs(title = "Session type distributions")

session_errors <- fr_en_md %>% 
  select(session, n_errors, user_id) %>% 
  group_by(session, user_id) %>% 
  summarise(avg_errors = mean(n_errors), .groups="drop") %>% 
  ggplot(aes(x=session, y=avg_errors))+
  geom_boxplot()+
  labs(title="Number of errors per session type", 
       y = "",
       x = "")+
  theme_minimal()

session_types + session_errors
```

It seems that learners are more careful when they see a new word or some unknown grammatical concept (`lesson`) than in situations when all the content is familiar to them (`practise`). In `test` sessions, a wider range in number of errors can be explained that both "experienced" and regular learners can take these tasks. In Section 3, I will concentrate on `practice` sessions to model users' mistakes in a "familiar" background.

### 2.3. Tasks and common mistakes

The app provides users with three different task formats: `listen` (listen and type a phrase in L2), `reverse_tap` (input L2 tokens in a correct order to translate a phrase) and `reverse_translate` (read and translate a phrase into L2). Only `listen` and `reverse_translate` tasks require typing, hence learners are more prone to make mistakes while taking them^[Indeed, minimal edit distance is used to handle mistyping but it depends on a token, e.g. *you* will not be accepted for *your* even if edit distance is only 1]. 

```{r, results='markup'}
fr_en_md %>%
  select(format, n_errors) %>% 
  group_by(format) %>% 
  summarise(avg_error = round(mean(n_errors),2)) %>% 
  knitr::kable(caption = "Average number of mistake users make per task",
               col.names = c("Task Type",
                             'Value'))
```

A task can contain from 1 to 14 tokens (depending on the language). Each token has a set of features assigned to it by @slam18 using the Google SyntaxNet dependency parser and the language-agnostic Universal Dependencies tagset^[Parse errors may occur.].

```{r, fig.width=10, fig.height=4}
pos_to_err_fr <- fr_en_sessions %>% 
  select(POS, label) %>%
  table()

pos_to_err_fr <- as.data.frame(pos_to_err_fr[,2] / (pos_to_err_fr[,1] + pos_to_err_fr[,2]))
colnames(pos_to_err_fr)[1] <- "error_rate"
pos_to_err_fr <- cbind(POS = as.factor(rownames(pos_to_err_fr)), pos_to_err_fr)
rownames(pos_to_err_fr) <- 1:nrow(pos_to_err_fr)

pos_to_err_fr %>%
  ggplot(aes(x=POS, y=error_rate))+
  geom_col() +
  geom_text(aes(label = round(error_rate,2)),
            position = position_dodge(width = 0.9), vjust = 1.3, color='white')+
  labs(title = 'Proportion of incorrectly inserted tokens to their total number of occurances',
       x = "", y="")+
  theme_minimal()
```

Top 3 most "erroneous" tags are `PUNCT`, `X`, `ADP`. The first UD tag refers to `-`, and this character is used in such questions as `Qui sont-ils?` or `Qu'a-t-il?`. The second question includes `t` character as well, which was tagged as `X`. Apparently, both cases, i.e. `PUNCT` and `X`, refer to word order issues as these mistakes happen a lot of the times in `reverse_tap` and `reverse_translate` tasks more often than in the others. Here, students have to input L2 sentences based on L1 prompts. In `listen` tasks, students can compare their input to the correct audio-prompt in L2.

```{r, results='markup'}
fr_en_md %>% 
  select(session_id, format) %>% 
  right_join(fr_en_sessions %>% 
              filter(POS %in% c('PUNCT','X') & label == 1) %>% 
              select(session_id) %>% distinct(), by='session_id') %>% 
  select(format) %>% 
  table() %>% 
  knitr::kable(col.names = c('Format', 'Number of Errors'),
               caption='Distribution of Errors Tagged as PUNCT by Task Format')
```

Based on this assumption, there is nothing else to learn about the nature of mistakes with first two "parts of speech". Some additional exploration may be performed for the third one, though. For example, the most "erroneous" word tagged as `ADP` is `de`. There are three variants of this preposition's spelling in Top 5 most common mistakes. 

```{r, results='markup'}
fr_en_sessions %>%
  select(POS, token, label) %>% 
  filter(POS %in% c('ADP') & label == 1) %>% 
  group_by(token) %>%
  summarise(counter = n()) %>% 
  slice_max(counter, n=10) %>% 
  pivot_wider(names_from = "token",
              values_from = "counter") %>% 
  add_column(Token = c('Quantity'), .before = "D'") %>% 
  knitr::kable(caption = 'Most common mistakes for the ADP tag')
```

It is quite unexpected, that `De` and `D'` are in the top of the list. It might refer to such phrases as `D'accord` (OK) and `De rien` (Not at all). The problem is that, without seeing actual users' input, it is difficult to understand if a wrong word was used "deliberately" or the users submitted their answer by accident. To avoid any bias, it was decided to explore only the prepositions `de`, `à`, `avec`, `en`, `comme` in more details.

```{r}
extract_trigrams <- function(df, token_ids) {
  # return a matrix of trigrams with token_ids being in the centre 
  output <- cbind(
      as.matrix(df)[cbind(1:nrow(df), token_ids-1)],
      as.matrix(df)[cbind(1:nrow(df), token_ids)],
      as.matrix(df)[cbind(1:nrow(df), token_ids+1)])
  
    return(
      as_tibble(output) %>% 
        rename(previous = V1, token=V2, following=V3)
      )
}

# handling diacritics of "a"
diacritics <- fr_en_sessions %>%
  select(POS, token, label) %>% 
  filter(POS %in% c('ADP')) %>% 
  select(token) %>% 
  distinct() %>% 
  slice(2:2) %>% 
  select(1)
```

```{r}
# extract trigrams from sentences where there are such these prepositions
sessions <- fr_en_sessions %>% 
  filter(POS == 'ADP' & token %in% c('de', 'avec', 'en',  "comme")| token == diacritics$token) %>% 
  select(session_id, token,task_token_id, label)

# some sentences have two mistakes, but will look only the first one.
sessions <- sessions[!duplicated(sessions$session_id),]
 
by_pos <- fr_en_sessions %>% 
  select(session_id, task_token_id, POS) %>% 
  filter(session_id %in% sessions$session_id) %>% 
  pivot_wider(names_from = "task_token_id",
              values_from = "POS")

pos2labels <- sessions %>% 
  select(session_id, label) %>% 
  cbind(
    extract_trigrams(
      by_pos %>% 
        select(!session_id),
      sessions$task_token_id)
    )
```

```{r, fig.height=3, fig.width=10}
pos2labels %>% 
  select(label, previous) %>% 
  mutate(error = ifelse(label == 1, "Mistake", 'Correct')) %>% 
  ggplot(aes(x=previous))+
  geom_bar(aes(fill = error), position = "dodge") +
  scale_fill_grey("", start=0.4, end=0.8)+
  labs(title = 'Proportion of mistakes for ADP token based on PoS of a previous token',
       x="", y="")+
  theme_minimal()
```

The graph above compares the number of correctly and incorrectly inserted prepositions if they are preceded by a certain part of speech. While exploring previous tokens , I found certain annotation errors that would impact SLAM if these morphological features were used as one of the variables. 

The example of these annotation errors may be seen if a preposition is preceded by a token with an `ADJ` tag.

```{r, results='markup'}
adj <- pos2labels %>% 
  filter(previous == 'ADJ') %>% 
  select(session_id) %>% 
  pull()

by_ud <- fr_en_sessions %>% 
  select(session_id, task_token_id, morph) %>% 
  filter(session_id %in% sessions$session_id) %>% 
  pivot_wider(names_from = "task_token_id",
              values_from = "morph")

ud2labels <- sessions %>% 
  select(session_id, label) %>% 
  cbind(
    extract_trigrams(
      by_ud %>% 
        select(!session_id),
     sessions$task_token_id))

ud2labels <- ud2labels %>% 
  select(1:3) %>% 
  filter(session_id %in% adj) %>% 
  select(session_id, label, previous) %>% 
  separate_rows(previous, sep = "\\|") %>% 
  separate(previous, into = c("feature", "value"), sep = "=")


features <- ud2labels %>% 
  pivot_wider(names_from = "feature",
              values_from = "value")

by_token <- fr_en_sessions %>% 
  select(session_id, task_token_id, token) %>% 
  filter(session_id %in% sessions$session_id) %>% 
  pivot_wider(names_from = "task_token_id",
              values_from = "token")

token2labels <- sessions %>% 
  select(session_id, label) %>% 
  cbind(
    extract_trigrams(
      by_token %>% 
        select(!session_id),
      sessions$task_token_id)
    )

options(knitr.kable.NA = '')

by_token %>% 
  inner_join(features %>% drop_na(Gender), by='session_id') %>% 
  select(2:7) %>% 
  slice(34:36)%>% 
  knitr::kable(caption="Examples of sentences which contain a preposition after an adjective")
```

Tag `ADJ` has the following feature distribution. Below are the example of prepositions preceded by `ADJ`.

```{r, results='markup'}
ud2labels %>% 
  select(feature, value) %>% 
  table() %>% 
  knitr::kable(caption="Feature distribution of ADJ", label="feature_distr")
```

There is only one binary feature with a large number of examples that we can use, for example, in a Chi-squared test: `fPOS`. The problem is that it is a a `fake` adjective feature. Its values are `ADJ++` and `VERB++`. The second group are the verbs, indeed. In fact, it is only one verb - `manger` (to eat) - in its `3^rd person singular` form.

```{r, results='markup'}
options(knitr.kable.NA = '')

by_token %>% 
  inner_join(features %>% filter(fPOS == 'VERB++') %>% drop_na(fPOS), by='session_id') %>% 
  select(2:9) %>% 
  drop_na(4:6)%>% 
  slice(27:30)%>%
  knitr::kable()
```


As there are more `fake` adjectives than `real` ones, and such proportion of mistakes might cause problems for modeling. I decided not to use morphological features for Second Language Acquisition Modeling performed in Chapter 3. 

## 3. Second Language Acqusition Modelling

As it was stated in Chapter 2.2, I am reducing the dataset to the `practice` session format to make sure learners are already familiar with all the vocabulary and grammatical concepts. 

A quick summary of the features:

1. On average, there are much more mistakes in `reverse_translate` and `listen` tasks than in `reverse_tap`.
2. Users have more problems with some tokens than with the others (though, it is not always clear why).
3. Previous and following tokens might be an extra feature but using UD tags seems to be unreliable.

A few additional ideas that might be tested as factors for formulas in mixed-effect models:

- task taken in the last decated of a 30-day period indicate that a user is commited to learn and might make less mistakes.

- It is easier to do some tasks (e.g. `reverse_tap`) using mobile platforms than browsers.

- It is easier to learn languages for some users than for the others.

To test for mix-effects models, I will use generalised mixed-effects modelling function from `lme4` package and a join of sessions' metadata and features of each token. 

``` {r, results='markup'}
df_cols = c('session_id', 'user_id', 'country', 'days', 'client',
            'session', 'format', 'time',
            'n_tokens', 'task_token_id', 'token',
            "previous_token", 'following_token', 'label')

description_df <- c(
  'uniqe session id',
  'generated during data anonimisation',
  'a 2-character country code',
  'day of usage (a double)',
  'android, ios, or web',
  'lesson, practice, or test',
  'reverse_translate, reverse_tap, or listen',
  'duration of the answer in seconds',
  'a number of tokens used in the task',
  'location of a token in a task',
  'token itself or the middle word in a trigram',
  'the first word in a trigram',
  'the last word in the trigram',
  'to be predicted (0 or 1): 0 - correct, 1 - wrong'
)

knitr::kable(data.frame(df_cols, description_df),
             col.names = c("Column name", 'Description'),
             align = c('l','l'),
             caption = "SLAM features")
```


```{r}
# # unncomment if file data/trigrams.csv does not exist
# trigrams <- fr_en_sessions %>%
#   select(session_id, task_token_id, token) %>%
#   pivot_wider(names_from = "task_token_id",
#               values_from = "token") %>%
#   group_by(session_id) %>%
#   unite(col = phrase, sep=" ", remove=FALSE) %>%
#   select(2,1) %>%
#   mutate(phrase = paste('NA', substring(phrase, 12), "NA")) %>%
#   unnest_tokens(ngram, phrase, token = "ngrams", n = 3, to_lower=FALSE) %>%
#   separate(ngram, into = c("previous", "token", 'following'), sep = " ", convert = TRUE)
# 
# trigrams <- trigrams[!with(trigrams,is.na(token) & is.na(following)),]
# 
# write_csv(trigrams, 'data/trigrams.csv', append = FALSE)
```


```{r}
trigrams <- read_csv("data/trigrams.csv",
                     col_types = cols('f','f','f','f'))
```

```{r}
# # unncomment if file data/df.csv does not exist
# df <- fr_en_sessions %>%
#   select(session_id, task_token_id, token, label) %>%
#   left_join(trigrams, by=c('session_id', 'token')) %>%
#   select(1,4,2,5,3,6) %>%
#   left_join(fr_en_md %>%
#               select(session_id, user_id, country, days, client,
#                      session, format, format,time, n_tokens), by='session_id')
# 
# write_csv(df, 'data/df.csv', append = FALSE)
```

```{r}
df <- read_csv("data/df.csv", col_types = cols('f','i','i',
                                               'c','f','c',
                                               'f','f','d',
                                               'f','f','f',
                                               'i','i'))

df[c("previous", "following")][is.na(df[c("previous", "following")])] <- "placeholder"

df <- df %>%
  filter(!is.na(time))

df <- df %>% 
  mutate(previous = as.factor(previous),
         following = as.factor(following)) %>% 
  filter(session == 'practice')
```

```{r}
# to test if there are not so many NAs
sum(!complete.cases(df))
```

In order to save on computational resources, I take only 1% of the data saving the share of `correct` and `incorrect` entries.

```{r}
# # uncomment if there is no data/sample_df.csv file.
# sample_df <- df %>%
#   slice_sample(prop = 0.01)
# 
# write_csv(sample_df, 'data/sample_df.csv', append = FALSE)
```

```{r, result='markup'}
sample_df <- read_csv("data/sample_df.csv", 
                      col_types = cols('f','i','i',
                                       'c','f','c',
                                       'f','f','d',
                                       'f','f','f',
                                       'i','i'))


sample_df %>% 
  group_by(label) %>% 
  summarise(n()) %>% 
  left_join(df %>% 
              select(label) %>%
              group_by(label) %>% 
              summarise(n()), by='label') %>% 
  knitr::kable(col.names = c("Label", 'Sample', 'Total'),
               align = c('c','r', 'r'),
               caption = "Number of entries by label")
```

I will begin with a few generalized linear mixed effects models assuming there is a random effect from a token `+ (1|token)`, a user `+ (1|user_id)`, a previous token `+ (1|previous)` or the following one `+ (1|following)`, `+ (1|country)`. First, I will test formulas that include numerical variables only: `n_tokens`, `days` and `task_token_id`.

- `Models 0-4`: label ~ n_tokens \* days \* task_token_id: the idea here is that the longer the sentence is, the more possibilities for mistake there are + further down the process of acquisition it is, the more committed the user is. If a token comes first, some mistakes happen by accident.

```{r, echo=TRUE, result='markup'}
lmer0 <- glmer(
  label~days*n_tokens*task_token_id + (1|token),
  data=sample_df, family = binomial)

lmer1 <- glmer(
  label~days*n_tokens*task_token_id + (1|user_id),
  data=sample_df, family = binomial)

lmer2 <- glmer(
  label~days*n_tokens*task_token_id + (1|previous),
  data=sample_df, family = binomial)

lmer3 <- glmer(
  label~days*n_tokens*task_token_id + (1|following),
  data=sample_df, family = binomial)

lmer4 <- glmer(
  label~days*n_tokens*task_token_id + (1|country),
  data=sample_df, family = binomial)
```

All of these models fail to converge as they are too complex. Further tests will use more simple models.

```{r, echo=TRUE, result='markup'}
lmer4 <- glmer(
  label~days + (1|user_id),
  data=sample_df, family = binomial)

lmer5 <- glmer(
  label~days + (1|token),
  data=sample_df, family = binomial)

lmer6 <- glmer(
  label~days + (1|country),
  data=sample_df, family = binomial)
```

```{r, results='markup'}
anova(lmer4, lmer5, lmer6)
```

All the models are very close and inefficient. Some improvement comes from a random effect by token itself but not enough.

```{r, results='markup'}
summary(lmer5)
```

```{r, echo=TRUE, results='markup'}
lmer7 <- glmer(
  label~n_tokens + (1|token),
  data=sample_df, family = binomial)

lmer8 <- glmer(
  label~task_token_id + (1|token),
  data=sample_df, family = binomial)

anova(lmer5, lmer7, lmer8)
```

Same thing for other fixed factor, still inefficient. It is now time to start testing categorical data with the same set of random effects.

```{r, echo=TRUE, results='markup'}
lmer9 <- glmer(
  label~format + format:client + (1|token),
  data=sample_df, family = binomial)

lmer10 <- glmer(
  label~format + format:client + (1|user_id),
  data=sample_df, family = binomial)

lmer11 <- glmer(
  label~format + format:client + (1|country),
  data=sample_df, family = binomial)

anova(lmer5, lmer9, lmer10, lmer11)
```
Model 9 `label ~ format + format:client + (1 | token)` looks more promising, yet it is still inefficient.

```{r, echo=TRUE, results='markup'}
summary(lmer9)
```
Probably, some improvement can be extracted from combining categorical and numerical variables.

```{r, echo=TRUE, results='markup'}
lmer11 <- glmer(
  label ~ days + format + (1 | token),
  data=sample_df, family = binomial)

lmer12 <- glmer(
  label ~ days + format + (1|user_id),
  data=sample_df, family = binomial)

lmer13 <- glmer(
  label ~ days + format + (1|country),
  data=sample_df, family = binomial)

anova(lmer11, lmer12, lmer13)
```

```{r, result='markup'}
summary(lmer11)
```

This set of feature does not provide any imporvement as well. For now and based on Chapter 2.3, it seems that choosing `correct` when a task format is `reverse_tap` is the most promising approach. Especially if grouped by the position of a token in a sentence.

```{r, echo=TRUE, results='markup'}
lmer17 <- glmer(
  label ~ format + 
    (1 + task_token_id|token),
  data=sample_df, family = binomial)

lmer18 <- glmer(
  label ~ n_tokens + 
    (1 + task_token_id|token),
  data=sample_df, family = binomial)

anova(lmer17, lmer18)
```

```{r, echo=TRUE, results='markup'}
summary(lmer17)
```

This feature might be useful to identify some accidental mistakes (see Chapter 2.3 on prepositions `De` and `D'`) but does not bring much to learning analytics.

## Conclusion

The project was aimed to explore if any available features have a stronger effect on `mistake`/ `correct` classification. While analysing the dataset, I removed UD features due to problems with tags. After I described the features, I applied generalised mixed-effects modeling to find out if any features or their combinations can be used to predict the label. The results of the experiments did not result in any meaningful feature set, and perhaps some additional synthesised features, e.g. `ngram frequency`, might be used in further experiments. Data sampling was applied mainly to save on computational resources, and as soon as some feature set is defined, it is possible to test it on larger dataset.

Below, there are features that I attempted to test during the experiments and alternative hypotheses for them:

* `days` - (fixed) the longer users study, the more committed they are (more attentive)

* `days:n_tokens` - Tasks' difficulty gradually increases, and it is possible that shorter sentences become easier with practice.

* `format` - average number of mistakes for `reverse_tap` tasks is three times as small as for the others.

* `format:client` - some tasks might be easier to do using cellphones rather than laptops or computers.

The following random effects were added to the model as well: `by token`, `by user`, `by country`. Their combination makes models too complex. 

## References
